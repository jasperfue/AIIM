{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 77823,
          "databundleVersionId": 8553100,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30698,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'aiim-emotion-classification:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F77823%2F8553100%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240529%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240529T111024Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7d64b51623dc44a5088e211860a2586cb5f4f65c4e268c21086797429908df19f4992ad676969a26719c005a2eb8a8876b719c0f1a7573663d9a9e07444cf1edc1c02a11f823e63ce8e17ae95d7e79305fe4c7f8e6efffdd3c4b65870a484c912b593549d83809ecde36f508e440796a311c81b30e2ddee0a842e2e1d1470db287f9050997a7addd702827308275d89b8bfdc6553c36f03dad48df92d95a621f3b52c96c9e69f34e7cab92a4bd1433d83251e7e651be7b88a0c4dc3db9d3c4cfe3aa6c2a341fdc340dd311142039410ca561656b3dcd760319a6783749f0d679709553ee7de36c3484da3cb4e24e792330366099ed7d2b4f1a810c44f39156ba'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szgufmtcC4Xy",
        "outputId": "951d70cf-8ac9-4aa2-b004-8b9a16fc13ff"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading aiim-emotion-classification, 18175099 bytes compressed\n",
            "[==================================================] 18175099 bytes downloaded\n",
            "Downloaded and uncompressed: aiim-emotion-classification\n",
            "Data source import complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import image_dataset_from_directory\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, RandomFlip, RandomRotation, RandomZoom, RandomContrast, RandomBrightness\n",
        "from keras.optimizers import Adam, SGD\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
      ],
      "metadata": {
        "is_executing": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmmAbR6hBp9t",
        "outputId": "009885d7-12a8-4ece-b03b-1d91d2b7183f"
      },
      "execution_count": 150
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "base_directory = '/kaggle/input/aiim-emotion-classification/aiim-emotion-classification/'\n",
        "batch_size = 32\n",
        "epochs = 20"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-29T11:01:32.566925Z",
          "start_time": "2024-05-29T11:01:32.563189Z"
        },
        "id": "TLPz1wGlBp9v"
      },
      "execution_count": 151
    },
    {
      "cell_type": "code",
      "source": [
        "# Documentation: https://keras.io/api/data_loading/image/\n",
        "train_ds, validation_ds = image_dataset_from_directory(\n",
        "    directory=base_directory + 'train/',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    color_mode='grayscale',\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    seed=420,\n",
        "    subset=\"both\",\n",
        "    validation_split=0.1,\n",
        "    image_size=(100, 100))\n",
        "\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "\n",
        "print(train_ds)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-27T22:45:25.809783Z",
          "iopub.execute_input": "2024-05-27T22:45:25.810281Z",
          "iopub.status.idle": "2024-05-27T22:45:27.818454Z",
          "shell.execute_reply.started": "2024-05-27T22:45:25.810244Z",
          "shell.execute_reply": "2024-05-27T22:45:27.817191Z"
        },
        "trusted": true,
        "ExecuteTime": {
          "end_time": "2024-05-29T11:01:33.015404Z",
          "start_time": "2024-05-29T11:01:32.568930Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0DvGi95Bp9w",
        "outputId": "f5f4ac7c-7681-47a0-86fc-e075d078bdb4"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9108 files belonging to 5 classes.\n",
            "Using 8198 files for training.\n",
            "Using 910 files for validation.\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 100, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5), dtype=tf.float32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "xU22__2yEhJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    RandomRotation(0.15),\n",
        "    RandomZoom(0.1),\n",
        "    RandomContrast(0.2),\n",
        "    RandomBrightness(0.2, value_range=(0, 1))\n",
        "])\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "validation_ds = validation_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "# AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "# validation_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-29T11:01:33.020603Z",
          "start_time": "2024-05-29T11:01:33.017424Z"
        },
        "id": "5HPvc0UJBp9w"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Operation\n",
        "\n",
        "## Convolutional Layer 1\n",
        "* Number of input Channels: 1 (Greyscale)\n",
        "* Number of output Channels: 32\n",
        "* Kernel size: 3x3\n",
        "* Stride: 1\n",
        "* Padding: 1\n",
        "\n",
        "## Max Pooling Layer 1\n",
        "* Kernel size: 2x2\n",
        "* Stride: 2\n",
        "* Padding: 0\n",
        "\n",
        "## Convolutional Layer 2\n",
        "* Number of input Channels: 32\n",
        "* Number of output Channels: 64\n",
        "* kernel size: 3 (3x3 kernel)\n",
        "* Stride: 1\n",
        "* Padding 1\n",
        "\n",
        "## Max Pooling Layer 2\n",
        "* Kernel size: 2x2\n",
        "* Stride: 2\n",
        "* Padding: 0\n",
        "\n",
        "## Flatten Layer\n",
        "Converts the multi-dimensional output of the Convolutional and Pooling layers into a 1D vector.\n",
        "\n",
        "## Fully Connected Layer 1\n",
        "* Input features: 64 * 25 * 25 (output size after the second pooling layer, flattened)\n",
        "* Output features: 128\n",
        "\n",
        "## Fully Connected Layer 2\n",
        "* Input features: 128\n",
        "* Output features: Number of classes (5 different emotions)"
      ],
      "metadata": {
        "id": "KxZOrtjoBp9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Normalization\n",
        "# model.add(normalization_layer)\n",
        "\n",
        "\n",
        "# Data Augmentation\n",
        "# model.add(data_augmentation)\n",
        "\n",
        "# Convolutional Layer 1\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='Same', activation='relu', input_shape=(100,100,1)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Convolutional Layer 2\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Convolutional Layer 3\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='Same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu', kernel_regularizer='l2'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5, activation='softmax'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-27T22:32:05.749473Z",
          "iopub.execute_input": "2024-05-27T22:32:05.749948Z",
          "iopub.status.idle": "2024-05-27T22:32:05.876351Z",
          "shell.execute_reply.started": "2024-05-27T22:32:05.749905Z",
          "shell.execute_reply": "2024-05-27T22:32:05.875032Z"
        },
        "trusted": true,
        "ExecuteTime": {
          "end_time": "2024-05-29T11:01:33.103456Z",
          "start_time": "2024-05-29T11:01:33.022638Z"
        },
        "id": "jDh-TMn3Bp9y"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try Learning rate of 0.001"
      ],
      "metadata": {
        "id": "tQ9q0dI3Bp9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "\n",
        "\n",
        "# Training des Modells\n",
        "history = model.fit(train_ds, validation_data=validation_ds, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, reduce_lr, model_checkpoint])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-27T22:32:05.879266Z",
          "iopub.execute_input": "2024-05-27T22:32:05.879695Z",
          "iopub.status.idle": "2024-05-27T22:44:47.637392Z",
          "shell.execute_reply.started": "2024-05-27T22:32:05.879623Z",
          "shell.execute_reply": "2024-05-27T22:44:47.636238Z"
        },
        "trusted": true,
        "is_executing": true,
        "ExecuteTime": {
          "start_time": "2024-05-29T11:01:33.105464Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-ADG7CSBp9z",
        "outputId": "376dc45e-164e-4c54-ca85-3b2bdf6f2dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "257/257 [==============================] - 14s 45ms/step - loss: 5.6855 - accuracy: 0.2181 - val_loss: 3.5124 - val_accuracy: 0.2319 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20\n",
            "257/257 [==============================] - 11s 41ms/step - loss: 2.1403 - accuracy: 0.2164 - val_loss: 1.7629 - val_accuracy: 0.2143 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "257/257 [==============================] - 9s 37ms/step - loss: 1.7346 - accuracy: 0.2157 - val_loss: 1.6274 - val_accuracy: 0.2198 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "257/257 [==============================] - 9s 35ms/step - loss: 1.6065 - accuracy: 0.2164 - val_loss: 1.5914 - val_accuracy: 0.2132 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "202/257 [======================>.......] - ETA: 2s - loss: 1.5958 - accuracy: 0.2169"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vorhersagen f√ºr die Validierungsdaten\n",
        "y_val_true = np.concatenate([y for x, y in validation_ds], axis=0)\n",
        "y_val_pred = model.predict(validation_ds)\n",
        "\n",
        "# Umwandeln der Vorhersagen in One-Hot-Format\n",
        "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
        "y_val_true_classes = np.argmax(y_val_true, axis=1)\n",
        "\n",
        "# F1-Score berechnen\n",
        "f1 = f1_score(y_val_true_classes, y_val_pred_classes, average='weighted')\n",
        "print(\"F1-Score: \", f1)"
      ],
      "metadata": {
        "id": "4HNlgG7KOYsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kppntn8sBp9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss and accuracy curves for training and validation\n",
        "plt.plot(history.history['val_loss'], color='b', label=\"validation loss\")\n",
        "plt.title(\"Test Loss\")\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-27T22:44:47.638812Z",
          "iopub.execute_input": "2024-05-27T22:44:47.639266Z",
          "iopub.status.idle": "2024-05-27T22:44:47.947255Z",
          "shell.execute_reply.started": "2024-05-27T22:44:47.639215Z",
          "shell.execute_reply": "2024-05-27T22:44:47.946049Z"
        },
        "trusted": true,
        "is_executing": true,
        "id": "dn7RMh5hBp9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Submission"
      ],
      "metadata": {
        "id": "NHwVgkLCBp90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = image_dataset_from_directory(\n",
        "    directory=base_directory + 'test/',\n",
        "    labels=None,\n",
        "    label_mode='categorical',\n",
        "    color_mode='grayscale',\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=69,\n",
        "    image_size=(100, 100))\n",
        "\n",
        "# Bilddateinamen extrahieren\n",
        "file_paths = test_ds.file_paths\n",
        "file_names = [file_path.split('/')[-1] for file_path in file_paths]\n",
        "\n",
        "test_ds = test_ds.map(lambda x: (normalization_layer(x)))\n",
        "\n",
        "\n",
        "y_test = model.predict(test_ds)\n",
        "\n",
        "\n",
        "y_pred_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Numerische Labels in Kategorienamen umwandeln\n",
        "y_pred_labels = [class_names[idx] for idx in y_pred_indices]\n",
        "\n",
        "# DataFrame erstellen\n",
        "df = pd.DataFrame({\n",
        "    'Id': file_names,\n",
        "    'emotions': y_pred_labels\n",
        "})\n",
        "\n",
        "# DataFrame als CSV speichern\n",
        "df.to_csv('/submission.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-27T22:44:47.948934Z",
          "iopub.execute_input": "2024-05-27T22:44:47.949415Z",
          "iopub.status.idle": "2024-05-27T22:44:55.557297Z",
          "shell.execute_reply.started": "2024-05-27T22:44:47.949373Z",
          "shell.execute_reply": "2024-05-27T22:44:55.556244Z"
        },
        "trusted": true,
        "is_executing": true,
        "id": "5-yfI29yBp90"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}